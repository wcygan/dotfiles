---
allowed-tools: Task, Read, Write, Edit, MultiEdit, Bash(fd:*), Bash(rg:*), Bash(eza:*), Bash(bat:*), Bash(jq:*), Bash(gdate:*), Bash(docker:*), Bash(kubectl:*), Bash(curl:*), Bash(git:*)
description: Intelligent integration orchestrator for services, APIs, databases, and tools with error handling and monitoring
---

## Context

- Session ID: !`gdate +%s%N 2>/dev/null || date +%s%N 2>/dev/null || echo "$(date +%s)$(jot -r 1 100000 999999 2>/dev/null || shuf -i 100000-999999 -n 1 2>/dev/null || echo $RANDOM$RANDOM)"`
- Integration target: $ARGUMENTS
- Current directory: !`pwd`
- Project structure: !`eza -la . --tree --level=2 2>/dev/null | head -10 || fd . -t d -d 2 | head -5`
- Existing integrations: !`fd "(client|connector|adapter|integration|api)" --type f -d 3 | head -10 || echo "No integration files detected"`
- Configuration files: !`fd "(.env|config|settings)" --type f -d 2 | head -5 || echo "No config files found"`
- Running services: !`docker ps --format "table {{.Names}}\t{{.Status}}" 2>/dev/null | head -5 || echo "No Docker services running"`
- Git status: !`git status --porcelain 2>/dev/null | head -5 || echo "Not a git repository"`

## Your Task

STEP 1: Initialize integration session and analyze existing architecture

- CREATE session state file: `/tmp/integration-session-$SESSION_ID.json`
- ANALYZE project structure and technology stack from Context section
- DETECT existing integrations, APIs, and service connections
- IDENTIFY integration complexity and requirements

```bash
# Initialize integration session state
echo '{
  "sessionId": "'$SESSION_ID'",
  "integrationTarget": "'$ARGUMENTS'",
  "existingIntegrations": [],
  "integrationPattern": "auto-detect",
  "securityProfile": "production"
}' > /tmp/integration-session-$SESSION_ID.json
```

STEP 2: Comprehensive integration analysis with parallel sub-agent coordination

TRY:

IF integration_complexity == "multi-service" OR target_includes_multiple_systems:

LAUNCH parallel sub-agents for comprehensive integration analysis:

- **Agent 1: API Discovery & Analysis**: Identify existing API endpoints, schemas, and authentication
  - Focus: REST APIs, GraphQL endpoints, RPC services, authentication mechanisms
  - Tools: rg for API patterns, fd for OpenAPI specs, curl for endpoint testing
  - Output: API inventory with capabilities and integration requirements

- **Agent 2: Database Integration Assessment**: Analyze database connections and data flows
  - Focus: Connection strings, ORM configurations, migration patterns, data schemas
  - Tools: rg for database patterns, analysis of config files and models
  - Output: Database integration strategy and data flow requirements

- **Agent 3: Service Discovery & Orchestration**: Map microservices and container architecture
  - Focus: Docker services, Kubernetes deployments, service mesh configuration
  - Tools: docker commands, kubectl queries, service discovery analysis
  - Output: Service topology and orchestration requirements

- **Agent 4: Security & Compliance Analysis**: Evaluate security requirements and patterns
  - Focus: Authentication, authorization, secrets management, encryption
  - Tools: rg for credential patterns, security configuration analysis
  - Output: Security integration requirements and compliance considerations

- **Agent 5: Monitoring & Observability Setup**: Design integration monitoring strategy
  - Focus: Logging, metrics, tracing, health checks, error handling
  - Tools: Analysis of existing monitoring patterns, log configurations
  - Output: Observability integration plan and alerting strategy

ELSE:

EXECUTE streamlined single-service integration analysis:

```bash
# Single-service integration analysis
echo "üîç Analyzing single-service integration requirements for: $ARGUMENTS"

# Identify existing patterns
rg "(http|https|api|client|sdk)" --type-add 'config:*.{json,yaml,toml}' --type config
fd "(client|connector|adapter|integration)" --type f

# Check for credentials and configuration
rg "(API_KEY|CLIENT_ID|SECRET|TOKEN)" .env* --no-ignore 2>/dev/null || echo "No credential files found"
rg "(endpoint|baseUrl|host|url)" --type-add 'config:*.{json,yaml,toml}' --type config

# Find current integration patterns
rg "(fetch|axios|request|WebClient|http\.)" -A 2 -B 1
```

STEP 3: Intelligent integration pattern selection and implementation

CASE integration_type:
WHEN "api_integration":

**REST API Integration (TypeScript/Deno optimized):**

```typescript
// api-integration.ts
interface APIConfig {
  baseUrl: string;
  apiKey?: string;
  timeout?: number;
  retryAttempts?: number;
}

class APIIntegration {
  constructor(private config: APIConfig) {}

  async request<T>(endpoint: string, options: RequestInit = {}): Promise<T> {
    const url = `${this.config.baseUrl}${endpoint}`;
    const headers = {
      "Content-Type": "application/json",
      ...(this.config.apiKey && { "Authorization": `Bearer ${this.config.apiKey}` }),
      ...options.headers,
    };

    let lastError: Error | null = null;

    for (let attempt = 0; attempt < (this.config.retryAttempts || 3); attempt++) {
      try {
        const response = await fetch(url, {
          ...options,
          headers,
          signal: AbortSignal.timeout(this.config.timeout || 30000),
        });

        if (!response.ok) {
          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }

        return await response.json();
      } catch (error) {
        lastError = error as Error;
        if (attempt < (this.config.retryAttempts || 3) - 1) {
          await new Promise((resolve) => setTimeout(resolve, Math.pow(2, attempt) * 1000));
        }
      }
    }

    throw lastError;
  }
}
```

WHEN "database_integration":

**Database Integration (Multi-database support):**

```typescript
// database-integration.ts
import { Client } from "https://deno.land/x/postgres/mod.ts";
import { connect } from "https://deno.land/x/redis/mod.ts";

class DatabaseIntegration {
  private pgClient: Client;
  private redisClient: Redis;

  async initialize() {
    // PostgreSQL connection
    this.pgClient = new Client({
      hostname: Deno.env.get("PG_HOST") || "localhost",
      port: parseInt(Deno.env.get("PG_PORT") || "5432"),
      user: Deno.env.get("PG_USER"),
      password: Deno.env.get("PG_PASSWORD"),
      database: Deno.env.get("PG_DATABASE"),
    });

    // Redis connection for caching
    this.redisClient = await connect({
      hostname: Deno.env.get("REDIS_HOST") || "localhost",
      port: parseInt(Deno.env.get("REDIS_PORT") || "6379"),
    });

    await this.pgClient.connect();
  }

  async query<T>(sql: string, params?: any[]): Promise<T[]> {
    const cacheKey = `query:${sql}:${JSON.stringify(params)}`;

    // Check cache first
    const cached = await this.redisClient.get(cacheKey);
    if (cached) {
      return JSON.parse(cached);
    }

    // Execute query
    const result = await this.pgClient.queryObject<T>(sql, params);

    // Cache result
    await this.redisClient.setex(cacheKey, 300, JSON.stringify(result.rows));

    return result.rows;
  }
}
```

WHEN "message_queue":

**Message Queue Integration (Go/RabbitMQ optimized):**

```go
// queue-integration.go
package integration

import (
    "github.com/rabbitmq/amqp091-go"
    "encoding/json"
    "time"
)

type QueueIntegration struct {
    conn    *amqp091.Connection
    channel *amqp091.Channel
}

func NewQueueIntegration(url string) (*QueueIntegration, error) {
    conn, err := amqp091.Dial(url)
    if err != nil {
        return nil, err
    }
    
    ch, err := conn.Channel()
    if err != nil {
        return nil, err
    }
    
    return &QueueIntegration{
        conn:    conn,
        channel: ch,
    }, nil
}

func (q *QueueIntegration) Publish(queue string, message interface{}) error {
    body, err := json.Marshal(message)
    if err != nil {
        return err
    }
    
    return q.channel.Publish(
        "",    // exchange
        queue, // routing key
        false, // mandatory
        false, // immediate
        amqp091.Publishing{
            ContentType: "application/json",
            Body:        body,
            Timestamp:   time.Now(),
        },
    )
}

func (q *QueueIntegration) Subscribe(queue string, handler func([]byte) error) error {
    msgs, err := q.channel.Consume(
        queue,
        "",    // consumer
        false, // auto-ack
        false, // exclusive
        false, // no-local
        false, // no-wait
        nil,   // args
    )
    if err != nil {
        return err
    }
    
    go func() {
        for msg := range msgs {
            if err := handler(msg.Body); err != nil {
                msg.Nack(false, true) // requeue on error
            } else {
                msg.Ack(false)
            }
        }
    }()
    
    return nil
}
```

WHEN "webhook_integration":

**Webhook Integration (Event-driven architecture):**

```typescript
// webhook-integration.ts
interface WebhookConfig {
  secret: string;
  endpoint: string;
  events: string[];
}

class WebhookIntegration {
  private handlers = new Map<string, Function[]>();

  constructor(private config: WebhookConfig) {}

  on(event: string, handler: Function) {
    if (!this.handlers.has(event)) {
      this.handlers.set(event, []);
    }
    this.handlers.get(event)!.push(handler);
  }

  async handleWebhook(req: Request): Promise<Response> {
    // Verify signature
    const signature = req.headers.get("X-Webhook-Signature");
    if (!this.verifySignature(await req.text(), signature)) {
      return new Response("Invalid signature", { status: 401 });
    }

    const payload = await req.json();
    const event = payload.event || req.headers.get("X-Event-Type");

    // Process event
    const handlers = this.handlers.get(event) || [];
    await Promise.all(handlers.map((h) => h(payload)));

    return new Response("OK", { status: 200 });
  }

  private verifySignature(body: string, signature: string | null): boolean {
    if (!signature) return false;

    const encoder = new TextEncoder();
    const key = encoder.encode(this.config.secret);
    const data = encoder.encode(body);

    // HMAC verification
    return crypto.subtle.verify(
      "HMAC",
      key,
      signature,
      data,
    );
  }
}
```

STEP 4: Service orchestration and workflow management

**Service Orchestration Engine:**

```typescript
// orchestration.ts
class ServiceOrchestrator {
  private services: Map<string, any> = new Map();

  register(name: string, service: any) {
    this.services.set(name, service);
  }

  async executeWorkflow(workflow: WorkflowDefinition) {
    const context: WorkflowContext = {
      data: {},
      errors: [],
    };

    for (const step of workflow.steps) {
      try {
        const service = this.services.get(step.service);
        if (!service) {
          throw new Error(`Service ${step.service} not found`);
        }

        const result = await service[step.method](...step.args);
        context.data[step.outputKey] = result;

        // Handle conditional branching
        if (step.condition && !step.condition(context)) {
          continue;
        }
      } catch (error) {
        context.errors.push({ step: step.name, error });

        if (step.onError === "fail") {
          throw error;
        } else if (step.onError === "skip") {
          continue;
        }
        // retry logic here
      }
    }

    return context;
  }
}
```

STEP 5: Authentication and authorization integration

**Authentication Integration (OAuth2/OIDC support):**

```typescript
// auth-integration.ts
class AuthIntegration {
  async integrateOAuth(provider: string) {
    const config = {
      google: {
        authUrl: "https://accounts.google.com/o/oauth2/v2/auth",
        tokenUrl: "https://oauth2.googleapis.com/token",
        scope: "openid email profile",
      },
      github: {
        authUrl: "https://github.com/login/oauth/authorize",
        tokenUrl: "https://github.com/login/oauth/access_token",
        scope: "read:user user:email",
      },
    };

    return {
      getAuthUrl(clientId: string, redirectUri: string) {
        const params = new URLSearchParams({
          client_id: clientId,
          redirect_uri: redirectUri,
          response_type: "code",
          scope: config[provider].scope,
        });

        return `${config[provider].authUrl}?${params}`;
      },

      async exchangeToken(code: string, clientId: string, clientSecret: string) {
        const response = await fetch(config[provider].tokenUrl, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            code,
            client_id: clientId,
            client_secret: clientSecret,
            grant_type: "authorization_code",
          }),
        });

        return response.json();
      },
    };
  }
}
```

STEP 6: Monitoring, observability, and error handling

**Monitoring Integration (Production-ready observability):**

```typescript
// monitoring-integration.ts
class MonitoringIntegration {
  constructor(
    private metrics: MetricsClient,
    private logger: Logger,
  ) {}

  wrapIntegration<T extends Function>(name: string, fn: T): T {
    return (async (...args: any[]) => {
      const start = Date.now();
      const labels = { integration: name };

      try {
        this.metrics.increment("integration.calls", labels);
        const result = await fn(...args);

        this.metrics.histogram("integration.duration", Date.now() - start, labels);
        this.metrics.increment("integration.success", labels);

        return result;
      } catch (error) {
        this.metrics.increment("integration.errors", labels);
        this.logger.error(`Integration ${name} failed`, { error, args });
        throw error;
      }
    }) as any;
  }
}
```

STEP 7: Generate production-ready integration artifacts

TRY:

**Generate Integration Documentation:**

```bash
# Create integration documentation
echo "üìã Generating integration documentation for: $ARGUMENTS"
```

**Integration Configuration Template:**

```yaml
# integration-config.yml
integration:
  name: "${ARGUMENTS//[^a-zA-Z0-9]/-}"
  source:
    type: "auto-detected"
    endpoint: "${SOURCE_ENDPOINT}"
    auth:
      type: "${AUTH_METHOD}"
      credentials: "${CREDENTIAL_SOURCE}"
  target:
    type: "auto-detected"
    endpoint: "${TARGET_ENDPOINT}"
    auth:
      type: "${AUTH_METHOD}"
      credentials: "${CREDENTIAL_SOURCE}"
  sync:
    strategy: "${real-time|batch|event-driven}"
    frequency: "${CRON_EXPRESSION}"
    batch_size: 100
    timeout: 30000
  monitoring:
    metrics: true
    logging: "structured"
    alerts:
      error_rate_threshold: 0.05
      latency_threshold: 5000
  resilience:
    retry:
      attempts: 3
      backoff: "exponential"
      base_delay: 1000
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      timeout: 60000
```

**Production Deployment Strategy:**

```bash
# Create deployment checklist
echo "üöÄ Production deployment checklist:"
echo "1. ‚úÖ Environment variables configured"
echo "2. ‚úÖ Authentication credentials secured"
echo "3. ‚úÖ Monitoring and alerting configured"
echo "4. ‚úÖ Error handling and retry logic implemented"
echo "5. ‚úÖ Integration tests passing"
echo "6. ‚úÖ Performance benchmarks validated"
echo "7. ‚úÖ Security review completed"
echo "8. ‚úÖ Documentation updated"
```

CATCH (integration_failed):

- LOG error details to session state
- PROVIDE alternative integration strategies
- SUGGEST troubleshooting steps

```bash
echo "‚ö†Ô∏è Integration setup failed. Analyzing issues..."
echo "Common integration issues:"
echo "  - Network connectivity problems"
echo "  - Authentication/authorization failures"
echo "  - API version mismatches"
echo "  - Configuration errors"
echo "  - Security policy restrictions"
```

STEP 8: Integration testing and validation

**Comprehensive Testing Strategy:**

```bash
# Connection testing
echo "üß™ Testing integration connectivity..."

# API endpoint validation
if command -v curl >/dev/null; then
  echo "Testing API endpoints with curl..."
fi

# Database connection testing
echo "Validating database connections..."

# Service discovery testing
if command -v docker >/dev/null; then
  echo "Testing service connectivity..."
  docker ps --format "table {{.Names}}\t{{.Status}}"
fi
```

**Integration Health Checks:**

```typescript
// health-check.ts
interface HealthCheckResult {
  service: string;
  status: "healthy" | "degraded" | "unhealthy";
  latency: number;
  timestamp: string;
  details?: string;
}

async function performHealthCheck(integration: string): Promise<HealthCheckResult> {
  const start = Date.now();

  try {
    // Perform actual health check based on integration type
    const response = await fetch(`${integration}/health`);
    const latency = Date.now() - start;

    return {
      service: integration,
      status: response.ok ? "healthy" : "degraded",
      latency,
      timestamp: new Date().toISOString(),
      details: response.ok ? undefined : `HTTP ${response.status}`,
    };
  } catch (error) {
    return {
      service: integration,
      status: "unhealthy",
      latency: Date.now() - start,
      timestamp: new Date().toISOString(),
      details: error.message,
    };
  }
}
```

STEP 9: Session state management and integration registry

**Update Session State:**

```bash
# Update integration session with results
jq --arg target "$ARGUMENTS" --arg timestamp "$(gdate -Iseconds 2>/dev/null || date -Iseconds)" '
  .integrationTarget = $target |
  .completedAt = $timestamp |
  .status = "completed"
' /tmp/integration-session-$SESSION_ID.json > /tmp/integration-session-$SESSION_ID.tmp && \
mv /tmp/integration-session-$SESSION_ID.tmp /tmp/integration-session-$SESSION_ID.json
```

**Integration Summary:**

```bash
echo "‚úÖ Integration completed successfully"
echo "üéØ Target: $ARGUMENTS"
echo "üìä Session: $SESSION_ID"
echo "‚è±Ô∏è Completed at: $(gdate -Iseconds 2>/dev/null || date -Iseconds)"
echo "üìÅ Artifacts: /tmp/integration-session-$SESSION_ID.json"
echo "üìñ Documentation: Available in project docs/"
```

FINALLY:

- SAVE session state and integration artifacts
- PROVIDE integration testing guidelines
- SUGGEST monitoring and maintenance procedures
- UPDATE project documentation with integration details

## Integration Patterns Reference

### Integration Types by Architecture

**Synchronous Patterns:**

- REST API integrations with request/response cycles
- Database query integrations with immediate results
- Microservice-to-microservice direct communication
- Real-time API aggregation and composition

**Asynchronous Patterns:**

- Event-driven architectures with message queues
- Webhook-based notification systems
- Batch processing integrations with scheduled execution
- Stream processing integrations for continuous data flow

**Hybrid Patterns:**

- Command Query Responsibility Segregation (CQRS)
- Event sourcing with read model projections
- Saga pattern for distributed transactions
- Circuit breaker pattern for resilient service communication

### Modern Integration Stack

**For Go Projects:**

- ConnectRPC for type-safe service communication
- GORM or sqlx for database integrations
- Go-kit for microservice patterns
- Prometheus for metrics integration

**For Rust Projects:**

- Axum for high-performance HTTP integrations
- SQLx for compile-time verified database queries
- Tokio for async runtime and networking
- Tracing for observability integration

**For TypeScript/Deno Projects:**

- Fresh for web application integrations
- Deno's built-in fetch for HTTP client integrations
- WebSocket API for real-time integrations
- Deno KV for simple state management

**For Java Projects:**

- Spring Boot with WebFlux for reactive integrations
- Spring Cloud for microservice patterns
- Quarkus for native compilation and performance
- Temporal for workflow orchestration

### Integration Best Practices

**Reliability:**

- Implement exponential backoff retry strategies
- Use circuit breakers for external service dependencies
- Design for idempotency in all integration endpoints
- Implement dead letter queues for failed message processing

**Security:**

- Use mutual TLS for service-to-service communication
- Implement OAuth2/OIDC for user context propagation
- Store secrets in dedicated secret management systems
- Validate and sanitize all external data inputs

**Observability:**

- Instrument all integration points with distributed tracing
- Implement structured logging with correlation IDs
- Monitor integration success rates, latency, and error patterns
- Set up alerting for integration health and performance degradation

**Performance:**

- Use connection pooling for database integrations
- Implement caching strategies for frequently accessed data
- Design for horizontal scaling with stateless integration services
- Use async patterns to prevent blocking operations

### Integration Testing Strategies

**Unit Testing:**

- Mock external dependencies for isolated testing
- Test error handling and retry logic
- Validate data transformation and mapping functions
- Test authentication and authorization flows

**Integration Testing:**

- Use TestContainers for database integration tests
- Test with real external services in staging environments
- Validate end-to-end data flow scenarios
- Test failure scenarios and recovery procedures

**Contract Testing:**

- Use Pact or similar tools for API contract validation
- Define and validate data schemas between services
- Test backward compatibility during service evolution
- Implement consumer-driven contract testing

This intelligent integration orchestrator adapts to your project architecture, leverages modern tools exclusively, and provides comprehensive integration capabilities through parallel sub-agent coordination when needed.

```
```
